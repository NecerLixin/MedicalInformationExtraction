{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from my_utils import NERDataset,collate_fn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[2584, 825, 2075, 1147, 1814, 1019, 122, 1515, 1147, 2448, 737, 2670, 926, 2448, 737, 2670, 1983, 970, 899, 564, 2075, 899, 1518, 737], [1147, 1814, 2670, 570, 2501, 2485, 737, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961], [2670, 2738, 1123, 2696, 2448, 737, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961], [2670, 2959, 2501, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961], [1983, 2738, 870, 2075, 864, 1983, 926, 899, 1518, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961], [2747, 1542, 2670, 2093, 210, 2448, 737, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961], [2670, 1983, 1549, 2604, 2696, 1455, 737, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961], [1983, 2670, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961, 2961]], [[10, 10, 10, 4, 9, 10, 10, 10, 4, 10, 10, 10, 4, 10, 10, 10, 10, 4, 9, 9, 10, 4, 9, 10], [4, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [10, 4, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [10, 4, 9, 10, 10, 10, 4, 4, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [10, 10, 10, 4, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [10, 10, 10, 10, 4, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]], [24, 7, 6, 3, 9, 7, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "dataset = NERDataset('nlp2024-data/dataset/dev.json')\n",
    "dataloader = DataLoader(dataset,batch_size=8,collate_fn=collate_fn)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获取所有字符\n",
    "data_dev = json.load(open('nlp2024-data/dataset/dev.json',encoding='utf-8'))\n",
    "data_train = json.load(open('nlp2024-data/dataset/train.json',encoding='utf-8'))\n",
    "data_test = json.load(open('nlp2024-data/dataset/test.json',encoding='utf-8'))\n",
    "def get_text_set(data):\n",
    "    text_set = set()\n",
    "    for key,val in data.items():\n",
    "        for character in val['diagnosis']:\n",
    "            text_set.add(character)\n",
    "        for character in val['self_report']:\n",
    "            text_set.add(character)\n",
    "        dialogues = val['dialogue']\n",
    "        for dia in dialogues:\n",
    "            for character in dia['speaker']:\n",
    "                text_set.add(character)\n",
    "            for character in dia['sentence']:\n",
    "                text_set.add(character)\n",
    "            for word in dia['symptom_norm']:\n",
    "                for character in word:\n",
    "                    text_set.add(character)\n",
    "    return text_set\n",
    "text_set_train = get_text_set(data_train)\n",
    "text_set_dev = get_text_set(data_dev)\n",
    "text_set_test = get_text_set(data_test)\n",
    "text_set = text_set_dev | text_set_train | text_set_test\n",
    "len(text_set)\n",
    "text_list = list(text_set)\n",
    "text_list.append('#')\n",
    "text_dict = {text_list[i]:i for i in range(len(text_list))}\n",
    "with open('char2id.json','w',encoding='utf-8') as f:\n",
    "    json.dump(text_dict,f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有的词\n",
    "def get_split_word(data):\n",
    "    word_set = set()\n",
    "    for key,val in data.items():\n",
    "        word_set = word_set | set(jieba.lcut(val['diagnosis']))\n",
    "        word_set = word_set | set(jieba.lcut(val['self_report']))\n",
    "        dialogues = val['dialogue']\n",
    "        for dia in dialogues:\n",
    "            word_set = word_set | set(jieba.lcut(dia['sentence']))\n",
    "            word_set = word_set | set(dia['symptom_norm'])\n",
    "    return word_set\n",
    "data_dev = json.load(open('nlp2024-data/dataset/dev.json',encoding='utf-8'))\n",
    "data_train = json.load(open('nlp2024-data/dataset/train.json',encoding='utf-8'))\n",
    "data_test = json.load(open('nlp2024-data/dataset/test.json',encoding='utf-8'))\n",
    "word_set_train = get_split_word(data_train)\n",
    "word_set_dev = get_split_word(data_dev)\n",
    "word_set_test = get_split_word(data_test)\n",
    "word_set = word_set_dev | word_set_train | word_set_test\n",
    "word_list = list(word_set)\n",
    "word_list.append('#')\n",
    "word_dict = {word_list[i]:i for i in range(len(word_list))}\n",
    "with open('word2id.json','w',encoding='utf-8') as f:\n",
    "    json.dump(word_dict,f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有的词\n",
    "word_dict = dict()\n",
    "def add_to_dict(l):\n",
    "    for w in l:\n",
    "        if w in word_dict:\n",
    "            word_dict[w] += 1\n",
    "        else:\n",
    "            word_dict[w] = 0\n",
    "def get_split_word(data):\n",
    "    word_set = set()\n",
    "    for key,val in data.items():\n",
    "        l =  jieba.lcut(val['diagnosis'])\n",
    "        add_to_dict(l)\n",
    "        l = jieba.lcut(val['self_report'])\n",
    "        add_to_dict(l)\n",
    "        dialogues = val['dialogue']\n",
    "        for dia in dialogues:\n",
    "            add_to_dict(jieba.lcut(dia['sentence']) + dia['symptom_norm'])\n",
    "data_dev = json.load(open('nlp2024-data/dataset/dev.json',encoding='utf-8'))\n",
    "data_train = json.load(open('nlp2024-data/dataset/train.json',encoding='utf-8'))\n",
    "data_test = json.load(open('nlp2024-data/dataset/test.json',encoding='utf-8'))\n",
    "word_set_train = get_split_word(data_train)\n",
    "word_set_dev = get_split_word(data_dev)\n",
    "word_set_test = get_split_word(data_test)\n",
    "len(word_dict)\n",
    "word_items = sorted(list(word_dict.items()),key=lambda x:x[1],reverse=True)\n",
    "word_set = {word_items[i][0] for i in range(len(word_items)) if word_items[i][1] > 15 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(word_set)\n",
    "word_list.append('#')\n",
    "word_dict = {word_list[i]:i for i in range(len(word_list))}\n",
    "with open('word2id.json','w',encoding='utf-8') as f:\n",
    "    json.dump(word_dict,f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lijinliang/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/lijinliang/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  872, 1962,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '你好'\n",
    "inputs = tokenizer(text,return_tensors='pt',padding=True,truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import NERDatasetBert, collate_fn_bert\n",
    "datasetBert = NERDatasetBert('nlp2024-data/dataset/dev.json',tokenizer)\n",
    "dataloader = DataLoader(datasetBert,batch_size=2, collate_fn=collate_fn_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.7955, -0.0604, -0.5145,  ..., -0.0065, -0.0573,  0.0602],\n",
      "         [ 0.0069, -0.1325,  0.1473,  ..., -0.6525, -0.4242,  0.1753],\n",
      "         [ 0.8572,  0.2717,  0.3732,  ..., -0.1347,  0.5311, -0.4706],\n",
      "         ...,\n",
      "         [ 0.6514, -0.5964, -0.2139,  ...,  0.6149,  0.1953,  0.6304],\n",
      "         [ 0.5118,  0.4172, -0.5241,  ..., -0.5596,  0.4079, -0.0406],\n",
      "         [ 0.0385, -0.4536, -0.0262,  ..., -0.3393, -0.2469,  0.2462]],\n",
      "\n",
      "        [[ 0.4655, -0.4097, -0.0354,  ...,  0.1277,  0.0083, -0.1648],\n",
      "         [ 0.0356, -0.2946, -0.5383,  ..., -0.3654, -0.5864,  0.3523],\n",
      "         [ 0.5439, -1.2687, -0.8399,  ...,  0.5940, -0.0624,  0.2715],\n",
      "         ...,\n",
      "         [ 0.2470, -0.0931, -0.1315,  ..., -0.3311, -0.1926, -0.1254],\n",
      "         [ 0.1547, -0.4363, -0.2219,  ..., -0.1255, -0.2254, -0.0903],\n",
      "         [ 0.1825, -0.3251, -0.1521,  ..., -0.1516, -0.2679, -0.1133]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9995,  0.9973,  0.9999,  ..., -0.8627, -0.9973,  0.6674],\n",
      "        [ 0.9999,  0.9983,  0.9999,  ..., -0.9174, -0.9977,  0.8010]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    inputs = {\"input_ids\":batch[0],\n",
    "              \"attention_mask\":batch[1]}\n",
    "    out = model(**inputs)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'norm': {0: '咳嗽',\n",
       "  1: '发热',\n",
       "  2: '感冒',\n",
       "  3: '鼻流涕',\n",
       "  4: '腹泻',\n",
       "  5: '痰',\n",
       "  6: '呕吐',\n",
       "  7: '中等度热',\n",
       "  8: '稀便',\n",
       "  9: '高热',\n",
       "  10: '消化不良',\n",
       "  11: '肺炎',\n",
       "  12: '支气管炎',\n",
       "  13: '鼻塞',\n",
       "  14: '病毒感染',\n",
       "  15: '低热',\n",
       "  16: '黄疸',\n",
       "  17: '细菌感染',\n",
       "  18: '哭闹',\n",
       "  19: '喷嚏',\n",
       "  20: '便秘',\n",
       "  21: '退热',\n",
       "  22: '呼吸道感染',\n",
       "  23: '腹胀',\n",
       "  24: '喘息',\n",
       "  25: '脱水',\n",
       "  26: '干咳',\n",
       "  27: '过敏',\n",
       "  28: '咽喉炎',\n",
       "  29: '绿便',\n",
       "  30: '精神软',\n",
       "  31: '皮疹',\n",
       "  32: '水样便',\n",
       "  33: '出汗',\n",
       "  34: '屁',\n",
       "  35: '上呼吸道感染',\n",
       "  36: '支气管肺炎',\n",
       "  37: '肠炎',\n",
       "  38: '支原体感染',\n",
       "  39: '腹痛',\n",
       "  40: '嗓子沙哑',\n",
       "  41: '大便粘液',\n",
       "  42: '痰鸣音',\n",
       "  43: '食欲不振',\n",
       "  44: '炎症',\n",
       "  45: '恶心',\n",
       "  46: '气喘',\n",
       "  47: '气管炎',\n",
       "  48: '咳痰',\n",
       "  49: '干呕',\n",
       "  50: '扁桃体炎',\n",
       "  51: '幼儿急疹',\n",
       "  52: '嗜睡',\n",
       "  53: '湿疹',\n",
       "  54: '尿量减少',\n",
       "  55: '上火',\n",
       "  56: '脓血便',\n",
       "  57: '抽搐',\n",
       "  58: '胃肠功能紊乱',\n",
       "  59: '四肢厥冷',\n",
       "  60: '肠鸣音',\n",
       "  61: '疱疹',\n",
       "  62: '头痛',\n",
       "  63: '鼻炎',\n",
       "  64: '缺钙',\n",
       "  65: '贫血',\n",
       "  66: '血便',\n",
       "  67: '过敏体质',\n",
       "  68: '咽喉不适',\n",
       "  69: '过敏性咳嗽',\n",
       "  70: '大便酸臭',\n",
       "  71: '呼吸困难',\n",
       "  72: '电解质紊乱',\n",
       "  73: '病理性黄疸',\n",
       "  74: '口腔溃疡',\n",
       "  75: '蛋花汤样便',\n",
       "  76: '烦躁',\n",
       "  77: '充血',\n",
       "  78: '啰音',\n",
       "  79: '声音嘶哑',\n",
       "  80: '轮状病毒感染',\n",
       "  81: '过敏性鼻炎',\n",
       "  82: '生理性黄疸',\n",
       "  83: '肠胃型感冒',\n",
       "  84: '咽部不适',\n",
       "  85: '哮喘',\n",
       "  86: '腹部不适',\n",
       "  87: '发黄',\n",
       "  88: '新生儿黄疸',\n",
       "  89: '胃肠道炎症',\n",
       "  90: '惊厥',\n",
       "  91: '粗糙呼吸音',\n",
       "  92: '脑炎',\n",
       "  93: '溶血性黄疸',\n",
       "  94: '呼吸急促',\n",
       "  95: '积食',\n",
       "  96: '口臭',\n",
       "  97: '喘憋',\n",
       "  98: '寒战',\n",
       "  99: '呃逆',\n",
       "  100: '粥样便',\n",
       "  101: '头晕',\n",
       "  102: '热惊厥',\n",
       "  103: '母乳性黄疸',\n",
       "  104: '呛奶',\n",
       "  105: '心肌炎',\n",
       "  106: '肠梗阻',\n",
       "  107: '胆红素脑病',\n",
       "  108: '淋巴结肿大',\n",
       "  109: '病毒性肠炎',\n",
       "  110: '鹅口疮',\n",
       "  111: '肺结核',\n",
       "  112: '皮肤黄',\n",
       "  113: '败血症',\n",
       "  114: '口周发青',\n",
       "  115: '鼻窦炎',\n",
       "  116: '细菌性肠炎',\n",
       "  117: '核黄疸',\n",
       "  118: '睡眠障碍',\n",
       "  119: '脑膜炎',\n",
       "  120: '手足口病',\n",
       "  121: '结膜炎',\n",
       "  122: '支原体肺炎',\n",
       "  123: '糊状便',\n",
       "  124: '大便干燥',\n",
       "  125: '咽峡炎',\n",
       "  126: '胆红素高',\n",
       "  127: '黑便',\n",
       "  128: '淋巴结肠炎',\n",
       "  129: '荨麻疹',\n",
       "  130: '肠痉挛',\n",
       "  131: '咳喘',\n",
       "  132: '眼发黄',\n",
       "  133: '支气管哮喘',\n",
       "  134: '乏力',\n",
       "  135: '中耳炎',\n",
       "  136: '肛裂',\n",
       "  137: '口吐白沫',\n",
       "  138: '痢疾',\n",
       "  139: '鼻流血',\n",
       "  140: '舌苔厚黄',\n",
       "  141: '尿路感染',\n",
       "  142: '心脏病',\n",
       "  143: '巨结肠',\n",
       "  144: '梗阻性黄疸',\n",
       "  145: '水泡',\n",
       "  146: '下呼吸道感染',\n",
       "  147: '溶血',\n",
       "  148: '喘鸣',\n",
       "  149: '乳糖不耐受',\n",
       "  150: '喉骨软化',\n",
       "  151: '流泪',\n",
       "  152: '新生儿肺炎',\n",
       "  153: '牙龈炎',\n",
       "  154: '感染',\n",
       "  155: '喘息性支气管炎',\n",
       "  156: '胆道闭锁',\n",
       "  157: '肠绞痛',\n",
       "  158: '急性胃肠炎',\n",
       "  159: '白血病',\n",
       "  160: '川崎病',\n",
       "  161: '室间隔缺损',\n",
       "  162: '麻疹',\n",
       "  163: '感染性黄疸',\n",
       "  164: '肝炎',\n",
       "  165: '慢性支气管炎',\n",
       "  166: '肠套叠',\n",
       "  167: '反胃',\n",
       "  168: '肛门红肿',\n",
       "  169: '畏寒',\n",
       "  170: '心率增快',\n",
       "  171: '阑尾炎',\n",
       "  172: '乳腺炎',\n",
       "  173: '盗汗',\n",
       "  174: '胸闷',\n",
       "  175: '窒息',\n",
       "  176: '病毒性皮疹',\n",
       "  177: '喘憋性肺炎',\n",
       "  178: '磨牙症',\n",
       "  179: '母乳性腹泻',\n",
       "  180: '尿痛',\n",
       "  181: '血尿',\n",
       "  182: '便隐血',\n",
       "  183: '缺锌',\n",
       "  184: '肌肉痉挛',\n",
       "  185: '超高热',\n",
       "  186: '打鼾',\n",
       "  187: '肺部感染',\n",
       "  188: '水痘',\n",
       "  189: '百日咳',\n",
       "  190: '气管支气管异物',\n",
       "  191: '淋巴结炎',\n",
       "  192: '四肢疼痛',\n",
       "  193: '脑病',\n",
       "  194: '猩红热',\n",
       "  195: '房间隔缺损',\n",
       "  196: '蚕豆病',\n",
       "  197: '痔疮',\n",
       "  198: '口唇发绀',\n",
       "  199: '肺纹理增多',\n",
       "  200: 'EB病毒感染',\n",
       "  201: '交叉感染',\n",
       "  202: '急性上呼吸道感染',\n",
       "  203: '胸痛',\n",
       "  204: '先天性脑发育不良',\n",
       "  205: '尿频尿急',\n",
       "  206: '体重减轻',\n",
       "  207: '流涎',\n",
       "  208: '尿量增多',\n",
       "  209: '脑出血',\n",
       "  210: '脱肛',\n",
       "  211: '衣原体感染',\n",
       "  212: '类百日咳综合征',\n",
       "  213: '脐周疼痛',\n",
       "  214: '溶血性黄胆',\n",
       "  215: '高胆红素血症',\n",
       "  216: '头颅外伤',\n",
       "  217: '血肿',\n",
       "  218: '低血糖',\n",
       "  219: '出血点',\n",
       "  220: '面色苍白',\n",
       "  221: '舌苔发白',\n",
       "  222: '肺炎支原体肺炎',\n",
       "  223: '胆道梗阻',\n",
       "  224: '舌苔发黑',\n",
       "  225: '昏迷',\n",
       "  226: '病毒性脑炎',\n",
       "  227: '癫痫',\n",
       "  228: '甲状腺功能减退',\n",
       "  229: '青铜症',\n",
       "  230: '风疹',\n",
       "  231: '颅内感染',\n",
       "  232: '心力衰竭',\n",
       "  233: '胸膜炎',\n",
       "  234: '皮屑',\n",
       "  235: '泪囊炎',\n",
       "  236: '佝偻病',\n",
       "  237: '过敏性支气管炎',\n",
       "  238: '过敏性皮疹',\n",
       "  239: '地中海贫血',\n",
       "  240: '中毒性脑病',\n",
       "  241: '高乳酸血症',\n",
       "  242: '硬块',\n",
       "  243: '粗呼吸音',\n",
       "  244: '咽喉痛',\n",
       "  245: '泡沫便',\n",
       "  246: '病毒性角膜炎',\n",
       "  247: '皮炎',\n",
       "  248: '脱发',\n",
       "  249: '紫绀',\n",
       "  250: '缺氧',\n",
       "  251: '过敏性紫癜',\n",
       "  252: '感染性心内膜炎',\n",
       "  253: '宫颈糜烂',\n",
       "  254: '痉挛',\n",
       "  255: '尿液浑浊',\n",
       "  256: '尿液异常',\n",
       "  257: '尿黄',\n",
       "  258: '肝功能异常',\n",
       "  259: '痒',\n",
       "  260: '发炎',\n",
       "  261: '艾滋病',\n",
       "  262: '关节疼痛',\n",
       "  263: '肌肉酸痛',\n",
       "  264: '高血压',\n",
       "  265: '糖尿病',\n",
       "  266: '胎盘早剥',\n",
       "  267: '喉水肿',\n",
       "  268: '肝胆堵塞',\n",
       "  269: '胆汁淤积',\n",
       "  270: '急性喉气管支气管炎',\n",
       "  271: '过敏性哮喘',\n",
       "  272: '右腿肿胀',\n",
       "  273: '脚痛',\n",
       "  274: '肠粘膜受损',\n",
       "  275: '气管支气管狭窄',\n",
       "  276: '半乳糖血症',\n",
       "  277: '肝炎病毒感染',\n",
       "  278: '血管瘤',\n",
       "  279: '阻塞性黄疸',\n",
       "  280: '眩晕',\n",
       "  281: '血小板增高症',\n",
       "  282: '晕厥',\n",
       "  283: '肺不张',\n",
       "  284: '肺纹理增粗',\n",
       "  285: '胃食管反流',\n",
       "  286: '脓肿',\n",
       "  287: '宫内感染',\n",
       "  288: '脐疝',\n",
       "  289: '鼻痒',\n",
       "  290: '接触性皮炎',\n",
       "  291: '胃溃疡',\n",
       "  292: '鞘膜积液',\n",
       "  293: '胸腔积液',\n",
       "  294: '睾丸炎',\n",
       "  295: '腰痛',\n",
       "  296: '脐带感染',\n",
       "  297: '牙痛',\n",
       "  298: '甲状腺功能亢进症',\n",
       "  299: '弓形体感染',\n",
       "  300: '先天性肠道闭锁',\n",
       "  301: '喉支气管炎',\n",
       "  302: '咯血',\n",
       "  303: '低钙血症',\n",
       "  304: '胃胀',\n",
       "  305: '肛门疼痛',\n",
       "  306: '颅内出血',\n",
       "  307: '功能性便秘',\n",
       "  308: '消化道出血',\n",
       "  309: '脓毒症',\n",
       "  310: '淋巴管瘤',\n",
       "  311: '风湿性关节炎',\n",
       "  312: '吞咽困难',\n",
       "  313: '腺样体肥大',\n",
       "  314: '巩膜黄染',\n",
       "  315: '预激综合征',\n",
       "  316: '呼吸衰竭',\n",
       "  317: '血小板减少症',\n",
       "  318: '胎膜早破',\n",
       "  319: '喷射性呕吐',\n",
       "  320: '禽流感',\n",
       "  321: '心悸',\n",
       "  322: '营养不良',\n",
       "  323: '心肌损伤',\n",
       "  324: '肾炎',\n",
       "  325: '急性肾炎',\n",
       "  326: '肾病综合征',\n",
       "  327: '鼻后漏综合征',\n",
       "  328: '肺炎喘嗽',\n",
       "  329: '肌张力增高',\n",
       "  330: '黄疸性肝炎'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('nlp2024-data/dataset/symptom_norm.csv')\n",
    "data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 1], 1: [1, 2], 2: [2, 4], 3: [4, 6]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('zh_core_web_lg')\n",
    "text = '我有一个苹果'\n",
    "doc = nlp(text)\n",
    "token_range = dict()\n",
    "for token in doc:\n",
    "    token_range[token.i] = [token.idx,token.idx + len(token.text)]\n",
    "token_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['欢迎', '来到', '华农']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "text = \"欢迎来到华农\"\n",
    "jieba.lcut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 我 dep 有 1\n",
      "1 1 有 ROOT 有 1\n",
      "2 2 一个 dep 有 1\n",
      "3 4 苹果 ROOT 苹果 4\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.i,token.idx,token.text,token.dep_,token.head,token.head.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_token_id = {}\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    token_id = token.i\n",
    "    start_char = token.idx\n",
    "    end_char = start_char + len(token.text)\n",
    "    for char_id in range(start_char, end_char):\n",
    "        char_to_token_id[char_id] = token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.tensor([[1,0,0],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[m==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified text: ['*', '磨', '牙', '*', '，', '晚', '上', '翻', '来', '覆', '去', '，', '大', '便', '干', '，', '吃', '的', '多', '，', '很', '容', '易', '*', '积', '食', '*']\n",
      "Entity positions: [[0, 3], [23, 26]]\n"
     ]
    }
   ],
   "source": [
    "def insert_asterisks(text, labels):\n",
    "    entity_pos = []\n",
    "    tokens = list(text)\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while(i < len(tokens)):\n",
    "        if labels[i]==\"B-Symptom\":\n",
    "            e = [len(new_tokens)]\n",
    "            new_tokens.append('*')\n",
    "            begin = i\n",
    "            end = i+1\n",
    "            while end < len(tokens) and labels[end]==\"I-Symptom\":\n",
    "                end += 1\n",
    "            e.append(e[0]+end-begin+1)\n",
    "            entity_pos.append(e)\n",
    "            for j in range(begin,end):\n",
    "                new_tokens.append(tokens[j])\n",
    "            new_tokens.append('*')\n",
    "            i = end-1\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return new_tokens,entity_pos\n",
    "\n",
    "text = \"磨牙，晚上翻来覆去，大便干，吃的多，很容易积食\"\n",
    "labels = \"B-Symptom I-Symptom O O O O O O O O O O O O O O O O O O O B-Symptom I-Symptom\".split()\n",
    "\n",
    "modified_text, entity_positions = insert_asterisks(text, labels)\n",
    "\n",
    "print(\"Modified text:\", modified_text)\n",
    "print(\"Entity positions:\", entity_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_text[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 加载模型和tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBartTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfnlp/bart-base-chinese\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m BartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfnlp/bart-base-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m text2text_generator \u001b[38;5;241m=\u001b[39m Text2TextGenerationPipeline(model, tokenizer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2048\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2045\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2046\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2287\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2285\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2287\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2292\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/bart/tokenization_bart.py:209\u001b[0m, in \u001b[0;36mBartTokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    207\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, T5ForConditionalGeneration, Text2TextGenerationPipeline\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "\n",
    "# 准备输入文本\n",
    "input_data = r\"\"\"\n",
    "    \"input\":\"我叫张三，今年45岁，最近咳嗽了很久，没有既往史，主诉：[MASK]，现病史：[MASK]\"\n",
    "\"\"\"\n",
    "# 将输入数据格式化为文本\n",
    "input_text = r\"\"\"\n",
    "{\n",
    "    \"主诉\": \"[MASK]\",\n",
    "    \"现病史\": \"[MASK]\",\n",
    "    \"辅助检查\": \"[MASK]\",\n",
    "    \"既往史\": \"[MASK]\",\n",
    "    \"诊断\": \"[MASK]\",\n",
    "    \"建议\": \"[MASK]\"\n",
    "            \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 生成医疗报告\n",
    "result = text2text_generator(input_data, max_length=2000,min_length=200, do_sample=False)\n",
    "\n",
    "# 打印生成的报告\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt','w') as f:\n",
    "    f.write(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "gpt2-chinese is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/gpt2-chinese/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1823\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66879ba3-2fa140ea64061c1c18ba2704;6a965a60-35e8-4d9c-9b8d-13d2fc7c70e3)\n\nRepository Not Found for url: https://huggingface.co/gpt2-chinese/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m\n\u001b[1;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m{\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 加载预训练的GPT-2模型和tokenizer\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2-chinese\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m生成医疗报告\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39minput_data, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1969\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   1967\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   1968\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 1969\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1979\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1980\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1981\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1983\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1984\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1986\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: gpt2-chinese is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "input_data = r\"\"\"\n",
    "{\n",
    "    \"input\":{\n",
    "{'title': '7个月的宝宝喉咙有痰十多天了，后面几天开始咳嗽，还有一天晚上烧到38度，去医院输了三天液之后咳嗽减轻了，但是痰还是很严重', 'context': '医生:您好，宝宝现在体温正常了吗？\n",
    "医生:宝宝嗓子哑吗？有没有气喘？什么时候咳嗽频繁？\n",
    "患者:现在正常了\n",
    "医生:好的\n",
    "患者:看了两个医生一个说是毛细支气管炎，一个说是支气管炎到支气管肺炎之间\n",
    "患者:没有住院\n",
    "医生:嗯嗯，现在还输液吗？\n",
    "患者:明天还要输一次\n",
    "患者:但我觉得没效果，要怎么办\n",
    "医生:宝宝咳嗽比之前是减轻了吧？\n",
    "患者:恩\n",
    "患者:减轻了\n",
    "患者:还有桔贝合剂\n",
    "医生:这个是中成药止咳化痰\n",
    "患者:氨茶碱片\n",
    "医生:这个是平喘药物\n",
    "患者:那还需要用什么药呢\n",
    "医生:输液是什么?_?\n",
    "医生:克林霉素就是控制感染的哦，喜炎平是止咳化痰的\n",
    "患者:谢谢了\n",
    "医生:积极配合医生治疗，跟医生多沟通孩子病情。\n",
    "医生:注意清淡饮食，给宝贝多喝水，避免呛咳\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "# 加载预训练的GPT-2模型和tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-chinese\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-chinese\")\n",
    "inputs = tokenizer.encode(\"生成医疗报告\"+input_data, return_tensors=\"pt\")\n",
    "\n",
    "# 生成文本\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=2000,   # 设置生成文本的最大长度\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    length_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印生成的报告\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
